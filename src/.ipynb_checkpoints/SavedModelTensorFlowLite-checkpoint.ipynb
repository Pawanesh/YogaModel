{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d953c54-5025-4ddf-99ba-a11cc376a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-18 21:52:27.446959: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-18 21:52:27.447850: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 160, 160, 3)]     0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "tf.math.truediv (TFOpLambda) (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "tf.math.subtract (TFOpLambda (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "mobilenetv2_1.00_160 (Functi (None, 5, 5, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 107)               137067    \n",
      "=================================================================\n",
      "Total params: 2,395,051\n",
      "Trainable params: 2,360,939\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "home = os.environ['HOME']\n",
    "modeldir = os.path.join(home, 'YogaModel/src')\n",
    "\n",
    "modelfile = os.path.join(modeldir, 'YogaModelTransferLearningFineTune.h5')\n",
    "load_model = tf.keras.models.load_model(modelfile)\n",
    "load_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1a916b1-9c1f-48cb-8af6-e815d9e7eff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-18 22:00:25.476240: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jupyter/YogaModel/SavedModel/YogaModel/1/assets\n"
     ]
    }
   ],
   "source": [
    "home = os.environ['HOME']\n",
    "saveddir = os.path.join(home, 'YogaModel/SavedModel/YogaModel/1')\n",
    "tf.saved_model.save(load_model, saveddir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "423a2e4a-462e-44f4-8601-f5c64918b7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-18 22:02:29.255836: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2021-11-18 22:02:29.255893: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n",
      "2021-11-18 22:02:29.255904: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored change_concat_input_ranges.\n",
      "2021-11-18 22:02:29.258211: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: /home/jupyter/YogaModel/SavedModel/YogaModel/1\n",
      "2021-11-18 22:02:29.309223: I tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\n",
      "2021-11-18 22:02:29.309276: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /home/jupyter/YogaModel/SavedModel/YogaModel/1\n",
      "2021-11-18 22:02:29.309455: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2021-11-18 22:02:29.549206: I tensorflow/cc/saved_model/loader.cc:211] Restoring SavedModel bundle.\n",
      "2021-11-18 22:02:30.609702: I tensorflow/cc/saved_model/loader.cc:195] Running initialization op on SavedModel bundle at path: /home/jupyter/YogaModel/SavedModel/YogaModel/1\n",
      "2021-11-18 22:02:30.792331: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 1534148 microseconds.\n",
      "2021-11-18 22:02:31.466189: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2021-11-18 22:02:32.099805: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(saveddir)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbec1209-2fb9-4f85-9eba-358bf7572c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9415460"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "tflite_model_file = pathlib.Path('YogaModel.tflite')\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3cd8758-6fad-4e1f-9f62-8ad7313bf55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "home = os.environ['HOME']\n",
    "tfliteDir = os.path.join(home, 'YogaModel/TFLiteModel')\n",
    "\n",
    "tfLiteFile = os.path.join(tfliteDir, 'YogaModel.tflite')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6b4cfb-f33a-4965-b53e-6438bd340272",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=tfLiteFile)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e17211db-46a6-4fe4-b6f2-28701dced22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'serving_default_input_2:0', 'index': 0, 'shape': array([  1, 160, 160,   3], dtype=int32), 'shape_signature': array([ -1, 160, 160,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "[{'name': 'StatefulPartitionedCall:0', 'index': 180, 'shape': array([  1, 107], dtype=int32), 'shape_signature': array([ -1, 107], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16cf553f-b0a1-4737-9b8a-8a6e9ac09363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'serving_default_input_2:0', 'index': 0, 'shape': array([  1, 160, 160,   3], dtype=int32), 'shape_signature': array([ -1, 160, 160,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "print(input_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba45174c-f54d-423d-b1c5-38d8f7742aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'StatefulPartitionedCall:0', 'index': 180, 'shape': array([  1, 107], dtype=int32), 'shape_signature': array([ -1, 107], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "print(output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ce723b0-4d7c-4100-9790-1d682bbf3351",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8d61157-ca5d-4755-8199-8ff186f9f289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4752 files belonging to 107 classes.\n",
      "Found 1241 files belonging to 107 classes.\n"
     ]
    }
   ],
   "source": [
    "PATH = '/home/jupyter/YogaModel/dataset/'\n",
    "train_dir = os.path.join(PATH, 'training')\n",
    "test_dir = os.path.join(PATH, 'testing')\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (160, 160)\n",
    "\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE\n",
    ")\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfa97e4f-a47f-4768-a6a6-fbc6b234a695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-18 22:47:28.569141: W tensorflow/core/lib/png/png_io.cc:88] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_batch, label_batch = validation_dataset.as_numpy_iterator().next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7c29c83-eb1b-4c74-a505-8a79deb1eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "test_labels, test_imgs = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2065128b-4d60-4394-b115-5dca3e11ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    img = image_batch[i]\n",
    "    label = label_batch[i]\n",
    "    \n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "    interpreter.set_tensor(input_index, img_array)\n",
    "    interpreter.invoke()\n",
    "    predictions.append(interpreter.get_tensor(output_index))\n",
    "    test_labels.append(label)\n",
    "    test_imgs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "461cda2a-8914-4dea-be09-67c7ce0b6e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label: 47\n",
      "predicted_label : 52\n",
      "Actual Label: 66\n",
      "predicted_label : 66\n",
      "Actual Label: 8\n",
      "predicted_label : 20\n",
      "Actual Label: 96\n",
      "predicted_label : 96\n",
      "Actual Label: 104\n",
      "predicted_label : 101\n",
      "Actual Label: 101\n",
      "predicted_label : 101\n",
      "Actual Label: 69\n",
      "predicted_label : 87\n",
      "Actual Label: 27\n",
      "predicted_label : 105\n",
      "Actual Label: 78\n",
      "predicted_label : 78\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(9):\n",
    "    prediction = predictions[i]\n",
    "    score = tf.nn.softmax(prediction[0])\n",
    "    label = test_labels[i]\n",
    "    print (\"Actual Label: \"+ str(label))\n",
    "    predicted_label = np.argmax(score)\n",
    "    print (\"predicted_label : \"+ str(predicted_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c891b4-799e-497b-b9c4-3a5f4cf74081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m84"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
